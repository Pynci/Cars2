behaviors:
  RedCar:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512               # batch grandi per stabilità
      buffer_size: 2048             # aggiornamenti più frequenti
      learning_rate: 3.0e-4         # buon compromesso tra velocità e stabilità
      learning_rate_schedule: linear
      beta: 1.0e-3                  # regolarizzazione dell'entropia
      epsilon: 0.2                  # clipping di PPO
      lambd: 0.95                   # GA‑lambda per l'estimate advantage
      num_epoch: 3                  # passaggi di ottimizzazione per buffer
    network_settings:
      normalize: true
      hidden_units: 512             # rete più ampia per task complesso
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
      curiosity:                    # segnale di esplorazione aggiuntivo
        gamma: 0.99
        strength: 0.1
    max_steps: 5000000             # ~5 milioni di passi, ovvero qualche ora
    time_horizon: 128              # orizzonte medio per bootstrapping
    summary_freq: 5000             # log ogni 5k passi
    keep_checkpoints: 5
    threaded: true

  BlueCar:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 2048
      learning_rate: 5.0e-4         # leggermente più alto per apprendimento rapido
      learning_rate_schedule: linear
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
    network_settings:
      normalize: true
      hidden_units: 256             # rete più leggera per baseline
      num_layers: 2
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 3000000             # 3 milioni di passi
    time_horizon: 128
    summary_freq: 5000
    keep_checkpoints: 3
    threaded: true
