behaviors:
  RedCar:
    trainer_type: sac
    hyperparameters:
      batch_size: 4096           # Manteniamo batch grandi per sfruttare la GPU
      buffer_size: 500000        # Buffer molto ampio, sfruttando off-policy
      learning_rate: 3.0e-4      # SAC richiede solitamente un learning rate stabile e pi√π basso
      init_entcoef: 0.2          # Coefficiente entropia iniziale (favorisce esplorazione, puoi sperimentare 0.1-0.5)
      reward_signal_steps_per_update: 1  # Frequenza aggiornamento reward signals
      tau: 0.005                 # Soft update target network, valore classico SAC
      steps_per_update: 1        # Aggiorna la policy ad ogni step raccolto
      save_replay_buffer: false  # Di norma non necessario salvare il buffer
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
      curiosity:
        gamma: 0.99
        strength: 0.02
        encoding_size: 256
        learning_rate: 1.0e-3
    keep_checkpoints: 10
    max_steps: 8.0e6
    time_horizon: 2000
    summary_freq: 25000
    checkpoint_interval: 200000

  BlueCar:
    trainer_type: sac
    hyperparameters:
      batch_size: 4096
      buffer_size: 500000
      learning_rate: 3.0e-4
      init_entcoef: 0.2
      reward_signal_steps_per_update: 1
      tau: 0.005
      steps_per_update: 1
      save_replay_buffer: false
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
      curiosity:
        gamma: 0.99
        strength: 0.02
        encoding_size: 256
        learning_rate: 1.0e-3
    keep_checkpoints: 10
    max_steps: 8.0e6
    time_horizon: 2000
    summary_freq: 25000
    checkpoint_interval: 200000

engine_settings:
  width: 84
  height: 84
  quality_level: 1
  time_scale: 20
  target_frame_rate: -1